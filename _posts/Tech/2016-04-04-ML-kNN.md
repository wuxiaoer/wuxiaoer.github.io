---
layout: post
title: 机器学习之kNN算法 
category: Tech 
tags: 
    - ML
    - kNN
keywords: machine learning, kNN
description:
---

# 机器学习之kNN
kNN是k-Nearest Neighbors的缩写, 即最近邻算法. kNN算法是比较容易理解的,也是非常有效的算法.

## KNN算法原理
距离测量进行分类
kNN算法
* 优点: 高准确率, 对异常值不敏感.
* 缺点: 计算量大, 需要大量内存空间
* 处理数据类型: 数字值,标称值

kNN算法的工作流程为:
训练集: 各个数据有一标签(Label), 即我们知道各数据属于哪一类别.

当给定新数据时(不知道标签), 与训练集进行比较, 找到最相似的数据(最近邻), 然后查看最相似的数据的标签, 找出前k个最相似的数据(k是一个整数值,而且通常k < 20).

最后, 对k个最相似的数据进行多数投票,  最多数据的类别就是新数据所属的类. 也即我们分类的结果.

以下给出一个非常简单的使用kNN算法的例子, 用来具体的看下kNN算法的实现

例子:  给电影分类, 动作片还是爱情片.

根据电影中打斗的次数和接吻的次数来判断一部电影是动作片还是爱情片

一般打斗的次数多的话,那么就是动作片
若接吻次数比较多的话, 那么就是爱情片

以下是训练集

![movie_classify](/public/img/movie_classify.png)

从图片中可知, 给出了6个训练集, 其中3部是爱情片, 3部是动作片. 最后一部是需要用来分类的电影.
给出了特征值(打斗次数和接吻次数), 根据前6部电影的特征以及类别,对当前电影进行判断.

判断的方法如下:
计算当前电影到其他电影特征的距离, 计算的距离如下表

![movie_distance](/public/img/movie_distance.png)

既然知道了当前要判断类别的电影到训练集中电影的距离, 然后按降序的方式对距离进行排序,找到前k个最仅的电影. 
此处假设k=3, 那么最接近的电影为:He’s Not Really into Dudes, Beautiful Woman和California Man
kNN算法从三部电影中进行多数投票判断当前电影的类别,  由于这三部电影都是爱情片, 因此, 判断当前电影为爱情片.

## KNN算法实现
以下通过使用python编程语言来实现一个简单的kNN算法

### 数据导入与预处理
使用python编程语言从文本文件中导入数据,并进行预处理.

此处非常的简单,并不是同文件中导入数据,而是通过数组导入. 代码如下:

{% highlight python %}
from numpy import *
import operator

def createDataSet():
    group  = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]])
    labels = ['A', 'A', 'B', 'B'] 
    return group, labels
{% endhighlight %}

该函数创建了数据集以及标签集

测试下该函数, 在终端中通过ipython交互界面,导入该函数,如下:
{% highlight python %}
fatansy@fantasy:kNN$ ipython
Python 2.7.8 (default, Jun 18 2015, 18:54:19) 
Type "copyright", "credits" or "license" for more information.

IPython 3.2.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: import createDataSet

In [2]: group, labels = createDataSet.createDataSet()

In [3]: group
Out[3]: 
array([[ 1. ,  1.1],
       [ 1. ,  1. ],
       [ 0. ,  0. ],
       [ 0. ,  0.1]])

In [4]: labels
Out[4]: ['A', 'A', 'B', 'B']

In [5]: 
{% endhighlight %}

### kNN分类算法实现

{% highlight python %}
# -*- coding: utf-8- -*-
import operator
from numpy import *

def kNN_classify(inX, dataSet, labels, k):
    dataSetSize = dataSet.shape[0]
    # 计算距离
    diffMat     = tile(inX, (dataSetSize, 1)) - dataSet
    sqDiffMat   = diffMat ** 2
    sqDistances = sqDiffMat.sum(axis = 1)
    distances   = sqDistances ** 0.5

    sortedDistIndicies = distances.argsort()
    
    classCount  = {}
    for i in range(k):
        voteLabel = labels[sortedDistIndicies[i]]
        classCount[voteLabel] = classCount.get(voteLabel, 0) + 1

    sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1), reverse = True)
    return sortedClassCount[0][0]
{% endhighlight %}

代码注释:

{% highlight python %}
shape用法:
用来计算矩阵的行和列, shape[0]返回行, shape[1] 返回列.

tile用法:
用来重复某个数组,  例如:tile([0, 0],  (4,1)) 以4行1列的方式进行重复
如下:
In [28]: tile([0, 0],  (4,1))
Out[28]: 
array([[0, 0],
       [0, 0],
       [0, 0],
       [0, 0]])

对于一维数组而言，只有axis=0可以使用（没必要使用）
对于二位数组而言，有axis=0或axis=1

sum(axis=1): 表示对矩阵的行向量进行相加

默认axis为None，表示将所有元素的值相加
对于二维数组
axis=1表示按行相加 , axis=0表示按列相加

argsort函数用法:
返回的是数组值从小到大的索引值
In [35]:  x = np.array([3, 1, 2])

In [39]: np.argsort(x)   #按升序排列
Out[39]: array([1, 2, 0])

In [40]: np.argsort(-x) #按降序排列
Out[40]: array([0, 2, 1])

In [41]: x[np.argsort(x)] #通过索引值排序后的数组
Out[41]: array([1, 2, 3])

In [42]: x[np.argsort(-x)]#通过索引值排序后的数组
Out[42]: array([3, 2, 1])

{% endhighlight %}

## 算法测试
对于输入inX = [0, 0]的测试结果

{% highlight python %}
fatansy@fantasy:kNN$ python
Python 2.7.8 (default, Jun 18 2015, 18:54:19) 
[GCC 4.9.1] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import createDataSet
>>> import kNN
>>> group, labels = createDataSet.createDataSet()
>>> k = 3
>>> inX = [0, 0]
>>> outY = kNN.kNN_classify(inX, group, labels, k)
>>> outY
'B'
>>> 
{% endhighlight %}

对于inX = [1.3, 1.5]的测试结果

{% highlight python %}
fatansy@fantasy:kNN$ python
Python 2.7.8 (default, Jun 18 2015, 18:54:19) 
[GCC 4.9.1] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import createDataSet
>>> import kNN
>>> group, labels = createDataSet.createDataSet()
>>> k = 3
>>> inX = [1.3, 1.5]
>>> outY = kNN.kNN_classify(inX, group, labels, k)
>>> outY
'A'
>>> 
{% endhighlight %}

从以上结果来看正确的.

为了测试分类算法的效果, 可以选择一些已知所属类的数据通过分类器算法进行分类, 记录下分类算法计算错误的次数, 除以整个的测试数据的个数, 就是错误率(error rate). 
错误率用来度量一个分类算法的好坏, 错误率为0意味着该分类算法是一个完美的分类器. 错误率为1.0 表示该分类器分类的结果总是错误的.

以下把kNN算法应用到实际的例子中,一个是通过kNN算法改善约会网站的结果. 另一个是手写数字的识别.

## kNN举例
1. 约会网站
2. 识别手写数字

## 参考

